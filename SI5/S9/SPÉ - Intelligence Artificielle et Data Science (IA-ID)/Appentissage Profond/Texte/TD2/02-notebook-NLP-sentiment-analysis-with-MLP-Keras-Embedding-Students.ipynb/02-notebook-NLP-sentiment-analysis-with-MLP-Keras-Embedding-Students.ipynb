{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Zc0WsSkoWK0"
   },
   "source": [
    "# Sentiment analysis with MLP and vector word representation (Keras embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-13T07:00:59.152715Z",
     "start_time": "2022-09-13T07:00:59.147557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"my_id_menu_nb\">run previous cell, wait for 2 seconds</div>\n",
       "<script>\n",
       "function repeat_indent_string(n){\n",
       "    var a = \"\" ;\n",
       "    for ( ; n > 0 ; --n)\n",
       "        a += \"    \";\n",
       "    return a;\n",
       "}\n",
       "// look up into all sections and builds an automated menu //\n",
       "var update_menu_string = function(begin, lfirst, llast, sformat, send, keep_item, begin_format, end_format) {\n",
       "    var anchors = document.getElementsByClassName(\"section\");\n",
       "    if (anchors.length == 0) {\n",
       "        anchors = document.getElementsByClassName(\"text_cell_render rendered_html\");\n",
       "    }\n",
       "    var i,t;\n",
       "    var text_menu = begin;\n",
       "    var text_memo = \"<pre>\\nlength:\" + anchors.length + \"\\n\";\n",
       "    var ind = \"\";\n",
       "    var memo_level = 1;\n",
       "    var href;\n",
       "    var tags = [];\n",
       "    var main_item = 0;\n",
       "    var format_open = 0;\n",
       "    for (i = 0; i <= llast; i++)\n",
       "        tags.push(\"h\" + i);\n",
       "\n",
       "    for (i = 0; i < anchors.length; i++) {\n",
       "        text_memo += \"**\" + anchors[i].id + \"--\\n\";\n",
       "\n",
       "        var child = null;\n",
       "        for(t = 0; t < tags.length; t++) {\n",
       "            var r = anchors[i].getElementsByTagName(tags[t]);\n",
       "            if (r.length > 0) {\n",
       "child = r[0];\n",
       "break;\n",
       "            }\n",
       "        }\n",
       "        if (child == null) {\n",
       "            text_memo += \"null\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        if (anchors[i].hasAttribute(\"id\")) {\n",
       "            // when converted in RST\n",
       "            href = anchors[i].id;\n",
       "            text_memo += \"#1-\" + href;\n",
       "            // passer à child suivant (le chercher)\n",
       "        }\n",
       "        else if (child.hasAttribute(\"id\")) {\n",
       "            // in a notebook\n",
       "            href = child.id;\n",
       "            text_memo += \"#2-\" + href;\n",
       "        }\n",
       "        else {\n",
       "            text_memo += \"#3-\" + \"*\" + \"\\n\";\n",
       "            continue;\n",
       "        }\n",
       "        var title = child.textContent;\n",
       "        var level = parseInt(child.tagName.substring(1,2));\n",
       "\n",
       "        text_memo += \"--\" + level + \"?\" + lfirst + \"--\" + title + \"\\n\";\n",
       "\n",
       "        if ((level < lfirst) || (level > llast)) {\n",
       "            continue ;\n",
       "        }\n",
       "        if (title.endsWith('¶')) {\n",
       "            title = title.substring(0,title.length-1).replace(\"<\", \"&lt;\")\n",
       "         .replace(\">\", \"&gt;\").replace(\"&\", \"&amp;\");\n",
       "        }\n",
       "        if (title.length == 0) {\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        while (level < memo_level) {\n",
       "            text_menu += end_format + \"</ul>\\n\";\n",
       "            format_open -= 1;\n",
       "            memo_level -= 1;\n",
       "        }\n",
       "        if (level == lfirst) {\n",
       "            main_item += 1;\n",
       "        }\n",
       "        if (keep_item != -1 && main_item != keep_item + 1) {\n",
       "            // alert(main_item + \" - \" + level + \" - \" + keep_item);\n",
       "            continue;\n",
       "        }\n",
       "        while (level > memo_level) {\n",
       "            text_menu += \"<ul>\\n\";\n",
       "            memo_level += 1;\n",
       "        }\n",
       "        text_menu += repeat_indent_string(level-2);\n",
       "        text_menu += begin_format + sformat.replace(\"__HREF__\", href).replace(\"__TITLE__\", title);\n",
       "        format_open += 1;\n",
       "    }\n",
       "    while (1 < memo_level) {\n",
       "        text_menu += end_format + \"</ul>\\n\";\n",
       "        memo_level -= 1;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    text_menu += send;\n",
       "    //text_menu += \"\\n\" + text_memo;\n",
       "\n",
       "    while (format_open > 0) {\n",
       "        text_menu += end_format;\n",
       "        format_open -= 1;\n",
       "    }\n",
       "    return text_menu;\n",
       "};\n",
       "var update_menu = function() {\n",
       "    var sbegin = \"\";\n",
       "    var sformat = '<a href=\"#__HREF__\">__TITLE__</a>';\n",
       "    var send = \"\";\n",
       "    var begin_format = '<li>';\n",
       "    var end_format = '</li>';\n",
       "    var keep_item = -1;\n",
       "    var text_menu = update_menu_string(sbegin, 2, 4, sformat, send, keep_item,\n",
       "       begin_format, end_format);\n",
       "    var menu = document.getElementById(\"my_id_menu_nb\");\n",
       "    menu.innerHTML=text_menu;\n",
       "};\n",
       "window.setTimeout(update_menu,2000);\n",
       "            </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(Practical tip) Table of contents can be compiled directly in jupyter notebooks using the following code:\n",
    "I set an exception: if the package is in your installation you can import it otherwise you download it \n",
    "then import it.\n",
    "\"\"\"\n",
    "try:\n",
    "    from jyquickhelper import add_notebook_menu \n",
    "except:\n",
    "    !pip install jyquickhelper\n",
    "    from jyquickhelper import add_notebook_menu\n",
    "    \n",
    "\"\"\"\n",
    "Output Table of contents to navigate easily in the notebook. \n",
    "For interested readers, the package also includes Ipython magic commands to go back to this cell\n",
    "wherever you are in the notebook to look for cells faster\n",
    "\"\"\"\n",
    "add_notebook_menu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, TextVectorization, Flatten, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSuNR5g2oUqH"
   },
   "source": [
    "In this lab we use part of the 'Amazon_Unlocked_Mobile.csv' dataset published by Kaggle. The dataset contain the following information:\n",
    "* Product Name\n",
    "* Brand Name\n",
    "* Price\n",
    "* Rating\n",
    "* Reviews\n",
    "* Review Votes\n",
    "\n",
    "We are mainly interested by the 'Reviews' (X) and by the 'Rating' (y)\n",
    "\n",
    "\n",
    "The goal is to try to predict the 'Rating' after reading the 'Reviews'. I've prepared for you TRAIN and TEST set.\n",
    "The work to be done is as follows:\n",
    "\n",
    "1. Feature extraction and baseline\n",
    "    * read the dataset and understand it\n",
    "    * put it in a format so that you can use `CountVectorizer` or`Tf-IDF` to extract the desired features\n",
    "    * perform on the desired dates and preprocessing\n",
    "    * use one of the classifiers you know to predict the polarity of different sentences\n",
    "1. My first neural network\n",
    "    * reuse the features already extracted \n",
    "    * proposed a neural network built with Keras\n",
    "1. Hyper-parameter fitting\n",
    "    * for the base line: adjust min_df, max_df, ngram, max_features + model's hyper-parameter\n",
    "    * for the neural network: adjust batch size, number of layers and number of neuron by layers, use earlystop\n",
    "1. <span style=\"color:red\">Word embedding\n",
    "    * stage 1 build a network that uses Keras' embedding which is not language sensitive.\n",
    "    * stage 2 build a network that simultaneously uses Keras' embedding and the features extracted in the first weeks.\n",
    "    * stage 3 try to use an existing embedding (https://github.com/facebookresearch/MUSE)\n",
    "    </span>\n",
    "\n",
    "**WARNING:** the dataset is voluminous, I can only encourage you to work first on a small part of it and only at the end, when the code is well debugged and that it is necessary to build the \"final model\", to use the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:51:59.283203Z",
     "start_time": "2022-01-25T18:51:58.181634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "gIXGN8IzoUqJ",
    "outputId": "f9cbed6e-5c8c-45cb-e083-4a9194f12157"
   },
   "outputs": [],
   "source": [
    "TRAIN = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/train.csv.gz\")\n",
    "VAL = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/val.csv.gz\")\n",
    "TEST = pd.read_csv(\"http://www.i3s.unice.fr/~riveill/dataset/Amazon_Unlocked_Mobile/test.csv.gz\")\n",
    "\n",
    "TRAIN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:51:59.343941Z",
     "start_time": "2022-01-25T18:51:59.302031Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WdeAXW40oUqJ",
    "outputId": "d99ac22b-c7e5-4c4a-f817-d8e25ff2a28c"
   },
   "outputs": [],
   "source": [
    "''' Construct X_train and y_train '''\n",
    "X_train = TRAIN['Reviews']\n",
    "y_train = np.array(TRAIN['Rating']).reshape(-1,1)\n",
    "X_val = VAL['Reviews']\n",
    "y_val = np.array(VAL['Rating'])\n",
    "X_test = TEST['Reviews']\n",
    "y_test = np.array(TEST['Rating']).reshape(-1,1)\n",
    "\n",
    "nb_classes = len(np.unique(y_train))\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "y_train_enc = ohe.fit_transform(y_train)\n",
    "y_val_enc = ohe.fit_transform(y_val)\n",
    "y_test_enc = ohe.fit_transform(y_test)\n",
    "\n",
    "X_train.shape, y_train_enc.shape, np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model (last week)\n",
    "\n",
    "$$ TO DO STUDENTS $$\n",
    "> * As the classes are unbalanced, it is preferable to use the `f1 score`.\n",
    "> * So modify your model, its compilation, its fit and its evaluation accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put here your previous preprocessing\n",
    "# Evaluate your baseline model with f1 score (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-03T18:00:17.888390Z",
     "start_time": "2021-01-03T18:00:17.884865Z"
    },
    "id": "SDWDQSbEoUqL"
   },
   "source": [
    "## Approach1 - BOW and MLP classifier (last week)\n",
    "\n",
    "The work was done last week but...\n",
    "$$ TO DO STUDENTS $$\n",
    "> * Create a vectorizer_layer with TextVectorization function\n",
    "> * Fit the vectorizer_layer (adapt function)\n",
    "> * Build an MLP and print the model (model.summary())\n",
    "> * Babysit your model\n",
    "> * Evaluate your model\n",
    ">\n",
    "> * As the classes are unbalanced, it is preferable to use the `f1 score`.\n",
    "> * So modify your model, its compilation, its fit and its evaluation accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:15.716618Z",
     "start_time": "2022-01-25T18:52:15.704147Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model with f1 metrics (Tensorflow f1 metrics or sklearn)\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4BohkzOoUqO"
   },
   "source": [
    "## Approach2 - Keras word embedding and MLP classifier\n",
    "\n",
    "Using the course companion notebook, build a multi-layer perceptron using an Embedding Keras layer and the same classifier as in approach 1. Evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Create a vectorizer_layer with TextVectorization function\n",
    "> * Fit the vectorizer_layer (adapt function)\n",
    "> * Build an MLP and print the model (model.summary())\n",
    "> * Babysit your model\n",
    "> * Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T16:07:40.982620Z",
     "start_time": "2022-01-22T16:07:40.978854Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = ... # fix your vocabulary size\n",
    "max_len = ...   # Sequence length to pad the outputs to.\n",
    "                # In order to fix it, you have to know the distribution on lengh... see first lab\n",
    "embed_dim = ... # embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ TO DO STUDENTS $$\n",
    "\n",
    ">* Create a vectorizer_layer with TextVectorization function\n",
    ">* Fit the vectorizer_layer (adapt function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:20.520759Z",
     "start_time": "2022-01-25T18:52:20.510528Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TO DO STUDENT$$\n",
    "\n",
    "> * Build an MLP and print the model (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:21.254213Z",
     "start_time": "2022-01-25T18:52:21.244686Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4BPv2OGoUqO",
    "outputId": "4cbf391a-81b9-447d-a0af-d6b469f85e2b"
   },
   "outputs": [],
   "source": [
    "# Your code... perhaps you need to use Flatten after Embedding in order to reduce the dimension of tensors\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ TO DO STUDENT $$\n",
    "> * Compile the network\n",
    "> * Fit the network using EarlyStopping\n",
    "> * Babysit your model\n",
    "> * Evaluate the network with f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:22.548119Z",
     "start_time": "2022-01-25T18:52:22.525400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7oN2bZkoUqP",
    "outputId": "8d7d5b26-b925-4e79-ad97-7aecd8980243"
   },
   "outputs": [],
   "source": [
    "# compile the model with metrics f1 score\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:23.009408Z",
     "start_time": "2022-01-25T18:52:23.002836Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "ZJjc2ISZoUqP",
    "outputId": "4585f6aa-bda8-4d82-885f-cece12094694"
   },
   "outputs": [],
   "source": [
    "# fit model using ealy stopping\n",
    "# Your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:23.547244Z",
     "start_time": "2022-01-25T18:52:23.306348Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1XWqf1QKoUqQ",
    "outputId": "3c73908b-3066-4c32-92f8-df458dc7f8cb"
   },
   "outputs": [],
   "source": [
    "# Babysit the model\n",
    "pd.DataFrame({'val_loss':history.history['val_loss'],\n",
    "              'loss':history.history['loss'],\n",
    "             'val_f1_score':history.history['val_f1_score'],\n",
    "              'f1_score':history.history['f1_score']}).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-25T18:52:23.656331Z",
     "start_time": "2022-01-25T18:52:23.650537Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3I9EmdInoUqQ",
    "outputId": "2b7de358-c515-4d89-e452-a63e38bc1577"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook-NLP-sentiment-analysis-with-MLP-Keras embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
